name: Data Pipeline

on:
  push:
    branches: [main]


jobs:
  build-and-run-pipeline:
    runs-on: ubuntu-latest
    env:
      EXCHANGE_API_KEY: ${{ secrets.EXCHANGE_API_KEY }}
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

    steps:
    - name: 1. Checkout code
      uses: actions/checkout@v4

    - name: 2. Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
        cache: 'pip' # Adiciona cache para acelerar instalações futuras

    - name: 3. Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Set PYTHONPATH
      run: echo "PYTHONPATH=$PYTHONPATH:$(pwd)/src" >> $GITHUB_ENV

    - name: 4. Run tests
      run: pytest tests/

    - name: 5. Run data pipeline for today
      run: python run_pipeline.py --date $(date +%F)

    - name: 6. Upload processed data as artifact
      uses: actions/upload-artifact@v4
      with:
        name: processed-data
        path: |
          raw/
          silver/
          gold/
          reports/

    - name: 7. Commit and push changes
      run: |
        git config --global user.name "github-actions[bot]"
        git config --global user.email "github-actions[bot]@users.noreply.github.com"
        git add raw/ silver/ gold/ reports/
        git commit -m "Update processed data and reports [skip ci]" || echo "No changes to commit"
        git push origin main
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

